
''' raise MissingInputError(error_msg, variable=r)
theano.gof.fg.MissingInputError: Input 0 of the graph (indices start from 0), used to compute Elemwise{neq,no_inplace}(/input_3, InplaceDimShuffle{x,x}.0), was not provided and not given a value. Use the Theano flag exception_verbosity='high', for more information on this error.
Backtrace when that variable is created:'''


from __future__ import print_function
import csv, datetime, time, json
from sklearn.model_selection import train_test_split
import sys

from keras.layers import Dense, Dropout, Input, Lambda,GRU,Embedding, LSTM

from keras.layers import Embedding
from keras.layers import Input, Conv2D, Lambda, merge, Dense, Flatten,MaxPooling2D
from keras.models import Model
import numpy as np

reload(sys)
sys.setdefaultencoding('utf-8')

LABEL_TRAINING_DATA_FILE = 'label_train.npy'
WORD_EMBEDDING_MATRIX_FILE = 'word_embedding_matrix.npy'
NB_WORDS_DATA_FILE = 'nb_words.json'
RNG_SEED = 13371447
VALIDATION_SPLIT = 0.02
TEST_SPLIT = 0.02
MAX_NB_WORDS = 200000
MAX_SEQUENCE_LENGTH = 25
EMBEDDING_DIM = 300

train_q1_data=np.load(open('train_Q1', 'rb'))
train_q2_data=np.load(open('train_Q2', 'rb'))
test_q1_data=np.load(open('test_Q1', 'rb'))
test_q2_data=np.load(open('test_Q2', 'rb'))
labels = np.load(open(LABEL_TRAINING_DATA_FILE, 'rb'))
word_embedding_matrix = np.load(open(WORD_EMBEDDING_MATRIX_FILE, 'rb'))
with open(NB_WORDS_DATA_FILE, 'r') as f:
    nb_words = json.load(f)['nb_words']


'''Creating The Training and Validation Set'''
X = np.stack((train_q1_data, train_q2_data), axis=1)
y = labels
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SPLIT, random_state=RNG_SEED)

Q1_train = X_train[:, 0]
Q2_train = X_train[:, 1]
Q1_test = X_test[:, 0]
Q2_test = X_test[:, 1]


input_a = Input(shape=(25,))
input_b = Input(shape=(25,))


input_sentence = Input(shape=(25,))
input_embedding = (Embedding(nb_words + 1, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, mask_zero=True, trainable=False,
                         weights=[word_embedding_matrix]))(input_sentence)
LSTM_layer = (LSTM(20, dropout=0.2, recurrent_dropout=0.2,activation='relu'))(input_embedding)


main1=Model(inputs=[input_sentence], outputs=[LSTM_layer])

model1=main1(input_a)
model2=main1(input_b)

concatenated = merge([model1, model2], mode='concat',concat_axis=1)
out = Dense(1, activation='sigmoid')(concatenated)

main_model = Model(input=[input_a,input_b], output=[out])

main_model.compile(loss='binary_crossentropy',
              optimizer='rmsprop',
              metrics=['accuracy'])

main_model.fit([Q1_train,Q2_train],y_train,nb_epoch=3, batch_size=128,verbose=1,shuffle=True,validation_data=([Q1_test,Q2_test],y_test))
