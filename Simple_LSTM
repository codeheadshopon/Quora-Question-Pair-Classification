from __future__ import print_function
import numpy as np
import csv, datetime, time, json
from zipfile import ZipFile
from os.path import expanduser, exists
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Embedding, Dense, Dropout, Reshape, Merge, BatchNormalization, TimeDistributed, Lambda
from keras.regularizers import l2
from keras.callbacks import Callback, ModelCheckpoint
from keras.utils.data_utils import get_file
from keras import backend as K
from sklearn.model_selection import train_test_split
import io
import shutil
from keras.layers import Dense, Embedding, Input
from keras.layers import LSTM
from keras.datasets import imdb
import sys
from keras.models import Sequential, Model
import sys

reload(sys)
sys.setdefaultencoding('utf-8')

GLOVE_ZIP_FILE_URL = 'http://nlp.stanford.edu/data/glove.840B.300d.zip'
MAX_NB_WORDS = 200000
MAX_SEQUENCE_LENGTH = 25
EMBEDDING_DIM = 300
VALIDATION_SPLIT = 0.1
TEST_SPLIT = 0.1
RNG_SEED = 13371447
NB_EPOCHS = 25

'''Reading Data From CSV Files'''
test_question1 = []
test_question2 = []

train_question1=[]
train_question2=[]
is_duplicate = []

with io.open('train.csv') as csvfile:
    reader = csv.DictReader(csvfile)
    for row in reader:
        train_question1.append(row['question1'])
        train_question2.append(row['question2'])
        is_duplicate.append(row['is_duplicate'])
with io.open('test.csv') as csvfile:
    reader = csv.DictReader(csvfile)
    for row in reader:
        test_question1.append(row['question1'])
        test_question2.append(row['question2'])

'''Tokenizing the Sentences ands Converting them to Sequences'''
questions = train_question1+train_question2+test_question1+test_question2
tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)
tokenizer.fit_on_texts(questions)
train_question1_word_sequences = tokenizer.texts_to_sequences(train_question1)
train_question2_word_sequences = tokenizer.texts_to_sequences(train_question2)
test_question1_word_sequences = tokenizer.texts_to_sequences(test_question1)
test_question2_word_sequences = tokenizer.texts_to_sequences(test_question2)
word_index = tokenizer.word_index


'''Using the GloVe Vector'''

embeddings_index = {}
with open('glove.840B.300d.txt') as f:
    for line in f:
        values = line.split(' ')
        word = values[0]
        embedding = np.asarray(values[1:], dtype='float32')
        embeddings_index[word] = embedding

nb_words = min(MAX_NB_WORDS, len(word_index))
word_embedding_matrix = np.zeros((nb_words + 1, EMBEDDING_DIM))
for word, i in word_index.items():
    if i > MAX_NB_WORDS:
        continue
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None:
        word_embedding_matrix[i] = embedding_vector


''' Padding all the vectors in same size'''
train_q1_data = pad_sequences(train_question1_word_sequences, maxlen=MAX_SEQUENCE_LENGTH)
train_q2_data = pad_sequences(train_question2_word_sequences, maxlen=MAX_SEQUENCE_LENGTH)
labels = np.array(is_duplicate, dtype=int)

test_q1_data = pad_sequences(test_question1_word_sequences, maxlen=MAX_SEQUENCE_LENGTH)
test_q2_data = pad_sequences(test_question2_word_sequences, maxlen=MAX_SEQUENCE_LENGTH)


'''Saving the numpy arrays for later usage'''
np.save(open('train_Q1', 'wb'), train_q1_data)
np.save(open('train_Q2', 'wb'), train_q2_data)
np.save(open('test_Q1', 'wb'), test_q1_data)
np.save(open('test_Q2', 'wb'), test_q2_data)
np.save(open('label_train.npy', 'wb'), labels)
np.save(open('word_embedding_matrix.npy', 'wb'), word_embedding_matrix)
with open('nb_words.json', 'w') as f:
    json.dump({'nb_words': nb_words}, f)


'''Creating The Training and Validation Set'''
X = np.stack((train_q1_data, train_q2_data), axis=1)
y = labels
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SPLIT, random_state=RNG_SEED)

Q1_train = X_train[:, 0]
Q2_train = X_train[:, 1]
Q1_test = X_test[:, 0]
Q2_test = X_test[:, 1]

Q1_test=test_q1_data
Q2_test=test_q2_data


'''Model Creation'''
def MODEL():
    Q1=Sequential()
    Q1.add(Embedding(nb_words + 1, EMBEDDING_DIM, weights=[word_embedding_matrix], input_length=MAX_SEQUENCE_LENGTH,
                     trainable=False))
    Q1.add(TimeDistributed(Dense(EMBEDDING_DIM, activation='relu')))
    return Q1

Q1=MODEL()
Q2=MODEL()

model = Sequential()
model.add(Merge([Q1, Q2], mode='concat'))
model.add(BatchNormalization())
model.add(LSTM(20, dropout=0.2, recurrent_dropout=0.2,activation='relu'))
model.add(Dense(1, activation='sigmoid'))


model.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

F3="weights-improvement-{epoch:02d}-{val_acc:.4f}.hdf5"
filepath=F3
callbacks = [ModelCheckpoint(filepath, monitor='val_acc', save_best_only=True)]
history = model.fit([Q1_train, Q2_train],
                    y_train,
                    nb_epoch=NB_EPOCHS,
                    validation_split=VALIDATION_SPLIT,
                    verbose=1,
                    callbacks=callbacks)

predicted = model.predict_proba([Q1_test,Q2_test])
